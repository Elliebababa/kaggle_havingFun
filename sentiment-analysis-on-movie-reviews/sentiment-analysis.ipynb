{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors...\n",
      "Found 400000 word vectors.\n",
      "17781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:255: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 60, 100)           1778100   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,895,993\n",
      "Trainable params: 1,895,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:295: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 60, 100)           1778100   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 2,013,881\n",
      "Trainable params: 2,013,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:329: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 60, 100)           1778100   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 60, 128)           117248    \n",
      "_________________________________________________________________\n",
      "att_1 (Att)                  (None, 128)               188       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 1,896,181\n",
      "Trainable params: 1,896,181\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "from keras.optimizers import SGD, Adam, Nadam, RMSprop\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.layers import Embedding,Conv1D,MaxPooling1D,Input\n",
    "from keras.layers.core import Dense, Activation,Dropout ,Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence,one_hot,Tokenizer\n",
    "from keras.constraints import maxnorm\n",
    "from keras.callbacks import ModelCheckpoint,TensorBoard, ReduceLROnPlateau,EarlyStopping\n",
    "from keras.applications import Xception\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import text\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def shuffle_2(a, b): # Shuffles 2 arrays with the same order\n",
    "    s = np.arange(a.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    return a[s], b[s]\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "\n",
    "DATA_DIR = '.\\\\data'\n",
    "GLOVE_DIR = os.path.join(DATA_DIR, 'glove6B')\n",
    "EMBEDDING_DIM = 100\n",
    "# # first, build index mapping words in the embeddings set\n",
    "# # to their embedding vector\n",
    "\n",
    "print('Indexing word vectors...')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'), encoding= 'utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "train_df = pd.read_csv('.\\\\data\\\\train.tsv',header=0,delimiter='\\t')\n",
    "test_df = pd.read_csv('.\\\\data\\\\test.tsv',header=0,delimiter='\\t')\n",
    "\n",
    "raw_docs_train = train_df['Phrase'].values\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "X_train = train_df['Phrase']\n",
    "Y_train = train_df['Sentiment']\n",
    "feature_names = train_df.columns.values\n",
    "X_test = test_df['Phrase']\n",
    "X_test_PhraseID = test_df['PhraseId']\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(np.concatenate((X_train, X_test), axis=0))\n",
    "Tokenizer_vocab_size = len(tokenizer.word_index) + 1\n",
    "print(Tokenizer_vocab_size)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#======================================== splitting data to train set and test set =====================================================\n",
    "\n",
    "num_test = 32000\n",
    "\n",
    "Y_Val = Y_train[:num_test]\n",
    "Y_Val2 = Y_train[:num_test]\n",
    "X_Val = X_train[:num_test]\n",
    "\n",
    "X_train = X_train[num_test:]\n",
    "Y_train = Y_train[num_test:]\n",
    "\n",
    "maxWordCount = 60\n",
    "maxDictionary_size = Tokenizer_vocab_size\n",
    "\n",
    "#======================================== to_sequences =====================================================\n",
    "\n",
    "encoded_words = tokenizer.texts_to_sequences(X_train)\n",
    "encoded_words2 = tokenizer.texts_to_sequences(X_Val)\n",
    "encoded_words3 = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "#======================================== padding =====================================================\n",
    "\n",
    "X_Train_encodedPadded_words = sequence.pad_sequences(encoded_words,maxlen = maxWordCount)\n",
    "X_Val_encodedPadded_words = sequence.pad_sequences(encoded_words2, maxlen=maxWordCount)\n",
    "X_test_encodedPadded_words = sequence.pad_sequences(encoded_words3, maxlen=maxWordCount)\n",
    "\n",
    "#======================================== one-hot labeling =====================================================\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train,5)\n",
    "Y_Val = keras.utils.to_categorical(Y_Val,5)\n",
    "\n",
    "#======================================== shuffling =====================================================\n",
    "\n",
    "shuffle_2(X_Train_encodedPadded_words,Y_train)\n",
    "\n",
    "#======================================== Embedding =============================================================\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,EMBEDDING_DIM,weights=[embedding_matrix],input_length=maxWordCount,trainable=True)\n",
    "\n",
    "\n",
    "#==================================================Attention Layer===========================================\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "\n",
    "class Att(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Att, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim\n",
    "       \n",
    "\n",
    "\n",
    "learning_rate = 0.0001\n",
    "epochs = 30\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "#========================================= model LSTM ================================================================\n",
    "\n",
    "tensorboard1 = keras.callbacks.TensorBoard(log_dir='./logs/log_1',histogram_freq=0,write_graph=True,write_images=False)\n",
    "checkpointer1 = ModelCheckpoint(filepath = \"./weights/weights_1\",verbose = 1, save_best_only = True, monitor = \"val_loss\")\n",
    "reducer_lr1 = ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=0,verbose = 1, mode = 'auto', cooldown = 0, min_lr = 1e-6)\n",
    "earlyStopping1 = EarlyStopping(monitor = 'val_loss',min_delta=0,patience=4,verbose=1)\n",
    "\n",
    "\n",
    "review_input = Input(shape = (60,),dtype = 'int32')\n",
    "#embeding_layer = Embedding(vocab_size, 128, dropout=0.2)\n",
    "embedded_sequence = embedding_layer(review_input)\n",
    "lstmLayer  = LSTM(128, dropout_W=0.2, dropout_U=0.2)#,return_sequences = True)\n",
    "#biLSTM = Bidirectional(LSTM(100,return_sequences = True))\n",
    "x = lstmLayer(embedded_sequence)\n",
    "#att = Att(60)(x)  \n",
    "denseLayer = Dense(5, activation='softmax')(x) \n",
    "\n",
    "model1 = Model(inputs=[review_input],outputs=denseLayer)\n",
    "model1.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "#====================================================training====================================================\n",
    "\n",
    "\n",
    "\n",
    "#history1  = model1.fit(X_Train_encodedPadded_words, Y_train, epochs = epochs, batch_size=batch_size, verbose=1, validation_data=(X_Val_encodedPadded_words, Y_Val), callbacks=[tensorboard1, reducer_lr1,checkpointer1,earlyStopping1])\n",
    "#scores1 = model1.evaluate(X_Val_encodedPadded_words, Y_Val, verbose=0)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model1,to_file='model1.png')\n",
    "\n",
    "#predicted_classes1 = np.argmax(model1.predict(X_test_encodedPadded_words,batch_size = batch_size,verbose =1),axis=1)\n",
    "#submission1=pd.DataFrame({'PhraseId':X_test_PhraseID,'Sentiment':predicted_classes1})\n",
    "#submission1.to_csv('./submission1.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#========================================= model BiLSTM================================================================\n",
    "\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "tensorboard2 = keras.callbacks.TensorBoard(log_dir='./logs/log_2',histogram_freq=0,write_graph=True,write_images=False)\n",
    "checkpointer2 = ModelCheckpoint(filepath = \"./weights/weights_2\",verbose = 1, save_best_only = True, monitor = \"val_loss\")\n",
    "reducer_lr2 = ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=0,verbose = 1, mode = 'auto', cooldown = 0, min_lr = 1e-6)\n",
    "earlyStopping2 = EarlyStopping(monitor = 'val_loss',min_delta=0,patience=4,verbose=1)\n",
    "\n",
    "\n",
    "review_input = Input(shape = (60,),dtype = 'int32')\n",
    "#embeding_layer = Embedding(vocab_size, 128, dropout=0.2)\n",
    "embedded_sequence = embedding_layer(review_input)\n",
    "bilstmLayer  = Bidirectional(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "x = bilstmLayer(embedded_sequence)\n",
    "denseLayer2 = Dense(5, activation='softmax')(x) \n",
    "\n",
    "model2 = Model(inputs=[review_input],outputs=denseLayer2)\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "model2.summary()\n",
    "\n",
    "#====================================================training====================================================\n",
    "\n",
    "#history2  = model2.fit(X_Train_encodedPadded_words, Y_train, epochs = epochs, batch_size=batch_size, verbose=1, validation_data=(X_Val_encodedPadded_words, Y_Val), callbacks=[tensorboard2, reducer_lr2,checkpointer2,earlyStopping2])\n",
    "#scores2 = model2.evaluate(X_Val_encodedPadded_words, Y_Val, verbose=0)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model1,to_file='model2.png')\n",
    "\n",
    "\n",
    "#predicted_classes2 = np.argmax(model2.predict(X_test_encodedPadded_words,batch_size = batch_size,verbose =1),axis=1)\n",
    "#submission2=pd.DataFrame({'PhraseId':X_test_PhraseID,'Sentiment':predicted_classes2})\n",
    "#submission2.to_csv('./submission2.csv',index=False)\n",
    "\n",
    "\n",
    "\n",
    "#========================================= model LSTM+ATTENTION================================================================\n",
    "\n",
    "tensorboard3 = keras.callbacks.TensorBoard(log_dir='./logs/log_3',histogram_freq=0,write_graph=True,write_images=False)\n",
    "checkpointer3 = ModelCheckpoint(filepath = \"./weights/weights_3\",verbose = 1, save_best_only = True, monitor = \"val_loss\")\n",
    "reducer_lr3 = ReduceLROnPlateau(monitor='val_loss',factor=0.8,patience=0,verbose = 1, mode = 'auto', cooldown = 0, min_lr = 1e-6)\n",
    "earlyStopping3 = EarlyStopping(monitor = 'val_loss',min_delta=0,patience=4,verbose=1)\n",
    "\n",
    "\n",
    "review_input = Input(shape = (60,),dtype = 'int32')\n",
    "#embeding_layer = Embedding(vocab_size, 128, dropout=0.2)\n",
    "embedded_sequence = embedding_layer(review_input)\n",
    "lstmLayer  = LSTM(128, dropout_W=0.2, dropout_U=0.2,return_sequences = True)\n",
    "#biLSTM = Bidirectional(LSTM(100,return_sequences = True))\n",
    "x = lstmLayer(embedded_sequence)\n",
    "att = Att(60)(x)  \n",
    "denseLayer3 = Dense(5, activation='softmax')(att) \n",
    "\n",
    "model3 = Model(inputs=[review_input],outputs=denseLayer3)\n",
    "model3.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics = ['accuracy'])\n",
    "model3.summary()\n",
    "\n",
    "#====================================================training====================================================\n",
    "\n",
    "#history3  = model3.fit(X_Train_encodedPadded_words, Y_train, epochs = epochs, batch_size=batch_size, verbose=1, validation_data=(X_Val_encodedPadded_words, Y_Val), callbacks=[tensorboard3, reducer_lr3,checkpointer3,earlyStopping3])\n",
    "#scores3 = model3.evaluate(X_Val_encodedPadded_words, Y_Val, verbose=0)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model1,to_file='model3.png')\n",
    "\n",
    "\n",
    "#predicted_classes3 = np.argmax(model3.predict(X_test_encodedPadded_words,batch_size = batch_size,verbose =1),axis=1)\n",
    "#submission3=pd.DataFrame({'PhraseId':X_test_PhraseID,'Sentiment':predicted_classes3})\n",
    "#submission3.to_csv('./submission3.csv',index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
